{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2478fbb8",
   "metadata": {},
   "source": [
    "### Pick a document\n",
    "\n",
    "Can be CSV, PDF, TXT, etc. or a website"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f374d874",
   "metadata": {},
   "source": [
    "#### PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fb6b2fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "pdf_path = \"context/Weller et al. - 2025 - On the Theoretical Limitations of Embedding-Based Retrieval.pdf\"\n",
    "\n",
    "loader = PyPDFLoader(\n",
    "    file_path=pdf_path,\n",
    "    mode=\"single\"\n",
    ")\n",
    "docs = loader.load()\n",
    "\n",
    "print(f\"Loaded PDF as {len(docs):d} document(s)\")\n",
    "print(\"-\"*50)\n",
    "print(\"Metadata:\")\n",
    "for k, v in docs[0].metadata.items():\n",
    "    print(f\"{k}: {v}\")\n",
    "print(\"-\"*50)\n",
    "print(\"Page content (preview):\")\n",
    "print(docs[0].page_content[:500] + \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c0d48c8",
   "metadata": {},
   "source": [
    "#### Wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "826a7ad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import WikipediaLoader\n",
    "\n",
    "loader = WikipediaLoader(\n",
    "    query=\"What is the capital of France?\",\n",
    "    load_max_docs=10\n",
    ")\n",
    "docs = loader.load()\n",
    "\n",
    "print(f\"Loaded {len(docs):d} document(s)\")\n",
    "print(\"-\"*50)\n",
    "for doc in docs:\n",
    "    print(doc.metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81055b20",
   "metadata": {},
   "source": [
    "#### Custom Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63006472",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.documents import Document\n",
    "\n",
    "page_content = \"Hello, world!\"\n",
    "\n",
    "docs = []\n",
    "doc = Document(\n",
    "    page_content=page_content,\n",
    "    metadata={\n",
    "        \"source\": \"My custom document\",\n",
    "        \"title\": \"Title of my custom document\"\n",
    "    }\n",
    ")\n",
    "docs.append(doc)\n",
    "\n",
    "print(\"-\"*50)\n",
    "print(\"Metadata:\")\n",
    "for k, v in docs[0].metadata.items():\n",
    "    print(f\"{k}: {v}\")\n",
    "print(\"-\"*50)\n",
    "print(\"Page content (preview):\")\n",
    "print(docs[0].page_content if len(docs[0].page_content) < 500 else docs[0].page_content[:500] + \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "244218c5",
   "metadata": {},
   "source": [
    "#### Website\n",
    "\n",
    "Caveat: Different websites have different structures and require different parsing strategies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f2dd8ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import bs4\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "import os\n",
    "import re\n",
    "os.environ['USER_AGENT'] = (\"Demo\")\n",
    "\n",
    "\"\"\"\n",
    "loader = WebBaseLoader(\n",
    "    web_paths=(\"https://www.pff.com/news/nfl-scores-and-recaps-for-every-week-3-game\",),\n",
    "    bs_kwargs=dict(\n",
    "        parse_only=bs4.SoupStrainer(\"article\")\n",
    "    ),\n",
    ")\n",
    "\"\"\"\n",
    "\n",
    "loader = WebBaseLoader(\n",
    "    web_paths=(\"https://www.cbssports.com/nfl/news/nfl-week-3-grades-scores-results-highlights-browns-packers-vikings-bengals/\",),\n",
    "    bs_kwargs=dict(\n",
    "        parse_only=bs4.SoupStrainer(\"article\")\n",
    "    ),\n",
    ")\n",
    "\n",
    "docs = loader.load()\n",
    "\n",
    "# Clean and condense page content: strip, remove blank lines, trim lines, and join into a single line\n",
    "docs[0].page_content = \" \".join(\n",
    "    line.strip() for line in docs[0].page_content.strip().splitlines() if line.strip()\n",
    ")\n",
    "\n",
    "# Replace sequences of two or more spaces with a single space\n",
    "docs[0].page_content = re.sub(r'\\s{2,}', ' ', docs[0].page_content)\n",
    "\n",
    "print(f\"Loaded Website as {len(docs):d} document(s)\")\n",
    "print(\"-\"*50)\n",
    "print(\"Metadata:\")\n",
    "for k, v in docs[0].metadata.items():\n",
    "    print(f\"{k}: {v}\")\n",
    "print(\"-\"*50)\n",
    "print(\"Page content (preview):\")\n",
    "print(docs[0].page_content if len(docs[0].page_content) < 10000 else docs[0].page_content[:10000] + \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f42b5991",
   "metadata": {},
   "source": [
    "### Chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "652cca65",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Split document into chunks for vector storage\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=500,\n",
    "    chunk_overlap=50,\n",
    "    add_start_index=True,\n",
    ")\n",
    "doc_chunks = text_splitter.split_documents(docs)\n",
    "\n",
    "print(f\"Document split into {len(doc_chunks):d} chunks.\")\n",
    "\n",
    "# Show first few chunks for verification\n",
    "for index, chunk in enumerate(doc_chunks[:3]):\n",
    "    print(f\"\\n\\nChunk {index+1}\")\n",
    "    print(\"=\"*50)\n",
    "    print(\"Metadata:\")\n",
    "    for k, v in chunk.metadata.items():\n",
    "        print(f\"{k}: {v}\")\n",
    "    print(\"-\"*50)\n",
    "    print(\"Page content:\")\n",
    "    print(chunk.page_content if len(chunk.page_content) < 500 else chunk.page_content[:500] + \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9128fad4",
   "metadata": {},
   "source": [
    "### Embed documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93a8cb3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#embedding_model_name = \"intfloat/multilingual-e5-base\"\n",
    "#embedding_model_name = \"Qwen/Qwen2.5-0.5B\"\n",
    "#embedding_model_name = \"Qwen/Qwen3-Embedding-0.6B\"\n",
    "embedding_model_name = \"Qwen/Qwen3-Embedding-4B\"\n",
    "#embedding_model_name = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "embedding_model_name = embedding_model_name\n",
    "\n",
    "embedding_function = HuggingFaceEmbeddings(\n",
    "    model_name=embedding_model_name,\n",
    "    model_kwargs={\"device\": \"cuda\"},\n",
    "    encode_kwargs={\"normalize_embeddings\": True},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f115d4be",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.vectorstores import InMemoryVectorStore\n",
    "\n",
    "vector_store = InMemoryVectorStore(embedding=embedding_function)\n",
    "\n",
    "# Add documents to vector store\n",
    "document_chunk_ids = vector_store.add_documents(documents=doc_chunks)\n",
    "\n",
    "print(f\"Added {len(document_chunk_ids):d} documents to the vector store\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddbcbc6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect vector store\n",
    "n_chunks = 5\n",
    "for index, (id, doc) in enumerate(vector_store.store.items()):\n",
    "    if index < n_chunks:\n",
    "        print(f\"Chunk {index+1}\")\n",
    "        print(\"-\"*50)\n",
    "        print(f\"id: {id}\")\n",
    "        print(f\"vector (length: {len(doc['vector'])}): {doc['vector']}\")\n",
    "        print(f\"metadata: {doc['metadata']}\")\n",
    "        print(f\"text:\\n{doc['text'] if len(doc['text']) < 100 else doc['text'][:100] + '...'}\\n\\n\")\n",
    "    else:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe022e70",
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss\n",
    "from langchain_community.docstore.in_memory import InMemoryDocstore\n",
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "vector_store_path = \"vector_store\"\n",
    "\n",
    "embedding_dim = len(embedding_function.embed_query(\"test\"))\n",
    "\n",
    "index = faiss.IndexFlatL2(embedding_dim)\n",
    "vector_store = FAISS(\n",
    "        embedding_function=embedding_function,\n",
    "        index=index,\n",
    "        docstore=InMemoryDocstore(),\n",
    "        index_to_docstore_id={},\n",
    "    )\n",
    "\n",
    "# Add documents to vector store\n",
    "document_chunk_ids = vector_store.add_documents(documents=doc_chunks)\n",
    "print(f\"Added {len(document_chunk_ids):d} documents to the vector store\")\n",
    "\n",
    "vector_store.save_local(vector_store_path)\n",
    "\n",
    "vector_store = FAISS.load_local(\n",
    "    vector_store_path, \n",
    "    embeddings=embedding_function,\n",
    "    allow_dangerous_deserialization=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a539d7a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect FAISS vector store\n",
    "n_chunks = 5\n",
    "print(f\"Total documents in FAISS vector store: {vector_store.index.ntotal}\")\n",
    "print(f\"Vector dimension: {vector_store.index.d}\")\n",
    "print(\"-\"*50)\n",
    "\n",
    "# Get document IDs from the index_to_docstore_id mapping\n",
    "docstore_ids = list(vector_store.index_to_docstore_id.values())[:n_chunks]\n",
    "\n",
    "for index, doc_id in enumerate(docstore_ids):\n",
    "    doc = vector_store.docstore.search(doc_id)\n",
    "    print(f\"Chunk {index+1}\")\n",
    "    print(\"-\"*50)\n",
    "    print(f\"Document ID: {doc_id}\")\n",
    "    print(f\"Metadata: {doc.metadata}\")\n",
    "    print(f\"Text:\\n{doc.page_content if len(doc.page_content) < 100 else doc.page_content[:100] + '...'}\\n\\n\")\n",
    "    if index >= n_chunks - 1:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1198d4e9",
   "metadata": {},
   "source": [
    "### Test vector store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f324d49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test vector store with a sample query\n",
    "query = \"Who was the Vikings' starting quarterback in week 3?\"\n",
    "\n",
    "top_k = 5\n",
    "similar_document_chunks = vector_store.similarity_search_with_score(query, k=top_k)\n",
    "\n",
    "print(f\"List of {len(similar_document_chunks):d} most similar document chunks for query: '{query:s}'\")\n",
    "\n",
    "for i, (doc, score) in enumerate(similar_document_chunks):\n",
    "    if i < top_k:\n",
    "        print(f\"Chunk {index+1}\")\n",
    "        print(\"-\"*50)\n",
    "        print(f\"id: {id}\")\n",
    "        print(f\"vector (length: {len(doc['vector'])}): {doc['vector']}\")\n",
    "        print(f\"metadata: {doc['metadata']}\")\n",
    "        print(f\"text:\\n{doc['text'] if len(doc['text']) < 100 else doc['text'][:100] + '...'}\\n\\n\")\n",
    "    else:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "589b6b3a",
   "metadata": {},
   "source": [
    "### Set up RAG query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec61a1aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "# Create prompt template for RAG system\n",
    "chat_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\",\n",
    "    \"\"\"\n",
    "    You are an AI assistant that answers questions based on provided context documents. \n",
    "    \"\"\"\n",
    "    ),\n",
    "    (\"human\",\n",
    "    \"\"\"\n",
    "    Answer the question based on the context.\n",
    "\n",
    "    CRITICAL RULES:\n",
    "    - Answer concisely\n",
    "    - Use information from the provided context\n",
    "    - If the context doesn't contain enough information, state this clearly\n",
    "    - Cite specific details from the context when possible\n",
    "\n",
    "    CONTEXT:\n",
    "    {context}\n",
    "\n",
    "    QUESTION: {input}\n",
    "\n",
    "    ANSWER:\n",
    "    \"\"\"\n",
    "    )\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7763a811",
   "metadata": {},
   "source": [
    "### Load LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70335759",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_huggingface import HuggingFacePipeline\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "\n",
    "model_name = \"google/gemma-3-1b-it\"\n",
    "print(f\"Loading llm <{model_name:s}>\")\n",
    "\n",
    "# Load llm\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "# Create text generation pipeline\n",
    "text_generation_pipeline = pipeline(\n",
    "    task=\"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    return_full_text=False,\n",
    "    max_new_tokens=250,    # Limit answer length for concise RAG responses\n",
    "    temperature=0.2,       # Low randomness, mostly deterministic\n",
    "    top_p=0.95,            # Sample from top 95% probable tokens\n",
    "    repetition_penalty=1.2 # Penalize repeated content to improve answer quality\n",
    ")\n",
    "\n",
    "# Wrap pipeline for LangChain\n",
    "llm = HuggingFacePipeline(pipeline=text_generation_pipeline)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b305c3f2",
   "metadata": {},
   "source": [
    "### The stuff documents chain\n",
    "\n",
    "Takes your dummy_docs (like Peter's first name + Peter's last name)\n",
    "Smooshes them all together into one big text\n",
    "\n",
    "Uses your chat_prompt (the template that says \"Hey AI, here's some context...\")\n",
    "Fills in the template with the smooshed-together documents\n",
    "\n",
    "Sends everything to the llm (your AI friend)\n",
    "Gets back a nice, smart answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a125bc30",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "\n",
    "combine_docs_chain = create_stuff_documents_chain(llm, chat_prompt)\n",
    "\n",
    "retriever = vector_store.as_retriever(\n",
    "    search_type=\"similarity\",\n",
    "    search_kwargs={\n",
    "        \"k\": 5,\n",
    "        \"score_threshold\": 0.7,\n",
    "    }\n",
    ")\n",
    "\n",
    "dummy_docs = [\n",
    "    Document(\n",
    "    page_content=\"My first name is Peter.\",\n",
    "    metadata={\"doc-nr.\": \"1\"}\n",
    "    ),\n",
    "    Document(\n",
    "    page_content=\"My last name is Parker.\",\n",
    "    metadata={\"doc-nr.\": \"2\"}\n",
    "    )\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd6b274b",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = combine_docs_chain.invoke({\"context\": dummy_docs, \"input\": \"What is the person's full name?\"})\n",
    "print(f\"Response:\")\n",
    "print(\"-\"*50)\n",
    "print(f\"{response:s}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e1e3d2d",
   "metadata": {},
   "source": [
    "### Retrieval chain\n",
    "\n",
    "We don't want ALL the context, just the most relevant parts\n",
    "\n",
    "So we use the retriever to get the most relevant parts\n",
    "Then we use the stuff documents chain to answer the question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79a3688d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import create_retrieval_chain\n",
    "\n",
    "retrieval_chain = create_retrieval_chain(retriever, combine_docs_chain)\n",
    "\n",
    "response = retrieval_chain.invoke({\"input\": \"What is this document about?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ca7867e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ANSWER\")\n",
    "print(\"-\"*50)\n",
    "print(f\"{response['answer']:s}\")\n",
    "print(\"\\n\\n\")\n",
    "\n",
    "print(\"CONTEXT\")\n",
    "print(\"-\"*50)\n",
    "for i, doc in enumerate(response['context']):\n",
    "    print(f\"Document {i+1}:\")\n",
    "    for key, value in doc.metadata.items():\n",
    "        print(f\"\\t{key}: {value}\")\n",
    "    print(f\"\\tPage content: {doc.page_content if len(doc.page_content) < 300 else doc.page_content[:300] + '...'}\")\n",
    "    print(\"\\n\")\n",
    "print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7fb5099",
   "metadata": {},
   "source": [
    "### Compare LLM vs RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbf8b24c",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"Who was the Vikings' starting quarterback in week 3?\"\n",
    "print(f\"Question: {question}\")\n",
    "print(\"\\n\")\n",
    "\n",
    "print(\"LLM without context:\")\n",
    "print(\"-\"*50)\n",
    "llm_response = llm.invoke(question)\n",
    "print(f\"Answer: {llm_response}\")\n",
    "print(\"\\n\\n\")\n",
    "\n",
    "print(\"RAG system with context:\")\n",
    "print(\"-\"*50)\n",
    "rag_response = retrieval_chain.invoke({\"input\": question})\n",
    "print(f\"Answer: {rag_response['answer']}\")\n",
    "\n",
    "print(\"Context:\")\n",
    "for i, doc in enumerate(rag_response['context']):\n",
    "    print(f\"Document {i+1}:\")\n",
    "    for key, value in doc.metadata.items():\n",
    "        print(f\"\\t{key}: {value}\")\n",
    "    print(f\"\\tPage content: {doc.page_content if len(doc.page_content) < 300 else doc.page_content[:300] + '...'}\")\n",
    "    print(\"\\n\")\n",
    "print(\"\\n\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ragnificent",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
